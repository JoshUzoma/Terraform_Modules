name: Terraform CI/CD with KICS Scanner and SonarQube

on:
#Trigger workflow on pushes to main branch, but only if user folders change
  push:
    branches: [main]
    paths:
      - 'user*/**'  # Triggers if any user folder is modified
  # Allow manual trigger from GitHub Actions UI
  workflow_dispatch:  # Manual trigger
  
env:
      AWS_REGION: us-east-2
      KICS_S3_BUCKET: kics-dev-bucket
      SQ_S3_BUCKET: sonarqube-log-bucket
      PROJECT_NAME: dea-diversions-sp-sonarqube
      LAMBDA_ARTIFACT_BUCKET: josh-dev-lambda-artifacts
      
jobs:

  # ===============================
  # 1. Detect which user folders changed
  # ===============================
  detect-folders:
    runs-on: ubuntu-latest
    outputs:
      folders: ${{ steps.set-folders.outputs.folders }}
    steps:
      - uses: actions/checkout@v4
        with:
           fetch-depth: 0 # Full History for git diff
           
      - id: set-folders
        run: |
          echo "Detecting user folders with changes..."
          folders=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep '^user' | cut -d/ -f1 | sort -u | uniq | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "folders=$folders" >> $GITHUB_OUTPUT

  # ===============================
  # 2. Run CI pipeline for each changed folder
  # ===============================        
  run-ci:
    name: Run Terraform CI/CD
    needs: detect-folders
    runs-on: ubuntu-latest
    if: needs.detect-folders.outputs.folders != '[]'
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    outputs:
      folder: ${{ matrix.folder }} 
    steps:

      # --- Checkout repo
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # --- Setup Terraform CLI    
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        
      # --- Configure AWS credentials  
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}  # Make this env variable
          
     # ===============================
     # Security Scanning - KICS
     # ===============================
      - name: Run KICS Scan
        uses: checkmarx/kics-github-action@v2.1.11
        with:
          path: '${{ matrix.folder }}'
          output_path: ${{ matrix.folder }}/kics-output
        continue-on-error: true  # Let pipeline continue even if KICS finds issues
        
      - name: Upload KICS Results
        uses: actions/upload-artifact@v4
        with:
          name: kics-results-${{ matrix.folder }}
          path: ${{ matrix.folder }}/kics-output
          
      - name: Upload KICS Results to S3
        run: |
          if ls ${{ matrix.folder }}/kics-output/* 1> /dev/null 2>&1; then
            timestamp=$(date +%Y%m%d-%H%M%S)
            for file in ${{ matrix.folder }}/kics-output/*; do
              filename=$(basename "$file")
              echo "Uploading $filename for ${{ matrix.folder }} to S3..."
              aws s3 cp "$file" \
                "s3://$KICS_S3_BUCKET/kics-results/${{ matrix.folder }}/$timestamp/$filename"
            done
          else
            echo "No KICS results found to upload."
          fi
          
       # Enforce security gates before apply
      - name: Fail on critical/high KICS issues
        run: |
           count=$(jq '[.queries[] | select(.severity == "HIGH" or .severity == "CRITICAL")] | length' ${{ matrix.folder }}/kics-output/results.json)
           echo "High/Critical findings: $count"
           if [ "$count" -gt 0 ]; then
             echo "High/Critical KICS findings — failing CI"
             exit 1
           fi

      # ---------------------------------------------------------
      # Step 1: Detect if this folder has any Lambda-related code
      # ---------------------------------------------------------   
      - name: Detect lambda content
        id: detect-lambda
        run: |
          cd ${{ matrix.folder }}
          if [ -d python ] || [ -d layers ]; then
            echo "has_lambda=true" >> $GITHUB_OUTPUT
          else
            echo "has_lambda=false" >> $GITHUB_OUTPUT
          fi
          
      # ---------------------------------------------------------
      # Step 2: Zip Lambda function code and/or layer code
      # ---------------------------------------------------------
      - name: Zip Lambda code and layers (reproducible)
        if: steps.detect-lambda.outputs.has_lambda == 'true'
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p build

          # Function to normalize file modification times for reproducibility
          normalize_dir() {
            local dir="$1"
            # Normalize mtimes of tracked files so zip timestamps are stable
            if git ls-files "$dir" >/dev/null 2>&1; then
              git ls-files "$dir" | xargs -I{} touch -t 200001010000 {}
            else
              # Fallback if directory isn’t tracked (rare)
              find "$dir" -type f -print0 | xargs -0 -I{} touch -t 200001010000 {}
            fi
          }

          # Function to zip contents of a directory in a stable, sorted way
          make_zip() {
            # make_zip <src_dir> <zip_out>
            local src="$1"
            local out="$2"
            # Build a sorted list of files, excluding noise
            mapfile -t files < <(cd "$src" && \
              find . -type f \
                ! -path "*/__pycache__/*" \
                ! -name "*.pyc" \
                ! -path "*/.DS_Store" \
                ! -path "*/.venv/*" \
              | LC_ALL=C sort)
            # Create the zip with stable attrs (-X) and deterministic file order
            rm -f "$out"
            (cd "$src" && zip -X -q "$OLDPWD/$out" "${files[@]}")
          }

          #Zip each Lambda function under python/
          if [ -d python ]; then
            for dir in python/*; do
              [ -d "$dir" ] || continue
              name=$(basename "$dir")
              normalize_dir "$dir"
              make_zip "$dir" "build/${name}.zip"
            done
          fi

          # Zip each Lambda layer under layers/
          if [ -d layers ]; then
            for dir in layers/*; do
              [ -d "$dir" ] || continue
              name=$(basename "$dir")
              normalize_dir "$dir"
              make_zip "$dir" "build/${name}_layer.zip"
            done
          fi
          
          echo "Built zips:"; ls -l build || true
          
      # ---------------------------------------------------------
      # Step 3: Upload artifacts to S3 and decide if TF should redeploy
      # ---------------------------------------------------------
      - name: Upload artifacts and write overrides (gated by content hash)
        if: steps.detect-lambda.outputs.has_lambda == 'true'
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out
          
          BUCKET="${{ env.LAMBDA_ARTIFACT_BUCKET }}"
          CHECKSUM_PREFIX="checksums"  # S3 folder to store per-function content hashes
          
          # Compute a stable content hash from source files under python/<func> (paths + file contents)
          content_hash() {
            local dir="$1"   # e.g., python/data_cleaner
            mapfile -t files < <(cd "$dir" && \
              find . -type f \
                ! -path "*/__pycache__/*" \
                ! -name "*.pyc" \
                ! -path "*/.DS_Store" \
                ! -path "*/.venv/*" \
              | LC_ALL=C sort)
            if [ "${#files[@]}" -eq 0 ]; then echo "EMPTY"; return; fi
            tmp=$(mktemp)
            (
              cd "$dir"
              for f in "${files[@]}"; do
                printf "%s\0" "$f"
                openssl dgst -sha256 -binary "$f" | openssl base64 -A
                printf "\n"
              done
            ) > "$tmp"
            openssl dgst -sha256 -binary "$tmp" | openssl base64 -A
            rm -f "$tmp"
          }
          
          # Upload and Lambda layers to S3 (no TF redeploy needed for layers)
          if compgen -G "build/*_layer.zip" > /dev/null; then
            for zip in build/*_layer.zip; do
              base="$(basename "$zip" .zip)"; name="${base%_layer}"
              aws s3 cp "$zip" "s3://${BUCKET}/layers/${name}.zip"
            done
          fi

          # Prepare overrides JSON for Terraform
          echo '{ "source_code_hashes": {' > tfvars_out/source_code.tfvars.json
          first=1
          
          # For each function zip, compare source content hash to prior run; only emit TF hash on change
          if compgen -G "build/*.zip" > /dev/null; then
            for zip in build/*.zip; do
              base="$(basename "$zip" .zip)"
              [[ "$base" == *_layer ]] && continue
              func="$base"
              src_dir="python/${func}"
              
              if [ ! -d "$src_dir" ]; then
                # Fallback: no source dir found — upload & emit once
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
                zip_hash=$(openssl dgst -sha256 -binary "$zip" | openssl base64 -A)
                if [ $first -eq 1 ]; then first=0; else echo "," >> tfvars_out/source_code.tfvars.json; fi
                echo "  \"${func}\": { \"source_code_hash\": \"${zip_hash}\" }" >> tfvars_out/source_code.tfvars.json
                continue
              fi

              # Compare new source hash to old one stored in S3
              new_ch=$(content_hash "$src_dir") || new_ch=""
              old_ch=""
              if aws s3 cp "s3://${BUCKET}/${CHECKSUM_PREFIX}/${func}.sha256" /tmp/prev.sha256 2>/dev/null; then
                old_ch="$(cat /tmp/prev.sha256 || true)"
              fi
              
              if [ -n "$new_ch" ] && [ "$new_ch" = "$old_ch" ]; then
                echo "No source change for ${func}; uploading zip but NOT emitting TF hash."
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
              else
                echo "Source change for ${func}; uploading & emitting TF hash."
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
                zip_hash=$(openssl dgst -sha256 -binary "$zip" | openssl base64 -A)
                if [ $first -eq 1 ]; then first=0; else echo "," >> tfvars_out/source_code.tfvars.json; fi
                echo "  \"${func}\": { \"source_code_hash\": \"${zip_hash}\" }" >> tfvars_out/source_code.tfvars.json
                # Save new content hash for next run
                printf "%s" "$new_ch" > /tmp/new.sha256
                aws s3 cp /tmp/new.sha256 "s3://${BUCKET}/${CHECKSUM_PREFIX}/${func}.sha256" >/dev/null
              fi
            done
          else
            echo "ERROR: no function zips found in build/" >&2
            exit 1
          fi
          
          echo "} }" >> tfvars_out/source_code.tfvars.json
          echo "=== overrides JSON ==="
          cat tfvars_out/source_code.tfvars.json

      # ---------------------------------------------------------
      # Step 4: If no Lambda code exists, make an empty overrides file
      # ---------------------------------------------------------
      - name: Write empty overrides (no Lambda in this folder)
        if: steps.detect-lambda.outputs.has_lambda != 'true'
        run: |
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out
          echo '{ "source_code_hashes": {} }' > tfvars_out/source_code.tfvars.json

      # ===============================
      # Terraform Plan Stage
      # ===============================
      - name: Upload tfvars_out to Artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out/source_code.tfvars.json
          
      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}
        
      - name: Terraform Validate
        run: terraform validate
        working-directory: ${{ matrix.folder }}
        
      - name: Terraform Plan
        run: terraform plan -var-file="vars/dev.tfvars" -var-file="tfvars_out/source_code.tfvars.json"
        working-directory: ${{ matrix.folder }}

  # ===============================
  # Terraform Apply (manual approval)
  # ===============================     
  terraform-apply:
    name: Terraform Apply
    needs: [run-ci, detect-folders]
    runs-on: ubuntu-latest
    environment:
      name: dev-apply  # Manual approval in GitHub Environments
      url: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Download tfvars_out from Artifact
        uses: actions/download-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out
          
      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}
        
      - name: Terraform Apply
        run: terraform apply -auto-approve -var-file="vars/dev.tfvars" -var-file="tfvars_out/source_code.tfvars.json"
        working-directory: ${{ matrix.folder }}
