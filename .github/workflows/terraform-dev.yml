name: Terraform CI/CD with KICS Scanner and SonarQube

on:
  push:
    branches: [main]
    paths:
      - 'user*/**'  # Triggers if any user folder is modified
  pull_request:
    branches: [main]
    paths:
      - 'user*/**' 
  workflow_dispatch:  # Manual trigger
  
env:
      AWS_REGION: us-east-2
      KICS_S3_BUCKET: kics-dev-bucket
      SQ_S3_BUCKET: sonarqube-log-bucket
      PROJECT_NAME: dea-diversions-sp-sonarqube
      LAMBDA_ARTIFACT_BUCKET: htown-dev-lambda-artifacts
      
jobs:
  detect-folders:
    runs-on: ubuntu-latest
    outputs:
      folders: ${{ steps.set-folders.outputs.folders }}
    steps:
      - uses: actions/checkout@v4
        with:
           fetch-depth: 0
           
      - id: set-folders
        run: |
          echo "Detecting user folders with changes..."
          folders=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep '^user' | cut -d/ -f1 | sort -u | uniq | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "folders=$folders" >> $GITHUB_OUTPUT
          
  run-ci:
    name: Run Terraform CI/CD
    needs: detect-folders
    runs-on: ubuntu-latest
    if: needs.detect-folders.outputs.folders != '[]'
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    outputs:
      folder: ${{ matrix.folder }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}  # Make this env variable
          
      # Static Code Scanning (Before Apply)
      - name: Run KICS Scan
        uses: checkmarx/kics-github-action@v2.1.11
        with:
          path: '${{ matrix.folder }}'
          output_path: ${{ matrix.folder }}/kics-output
        continue-on-error: true  # Let pipeline continue even if KICS finds issues
        
      - name: Upload KICS Results
        uses: actions/upload-artifact@v4
        with:
          name: kics-results-${{ matrix.folder }}
          path: ${{ matrix.folder }}/kics-output
          
      - name: Upload KICS Results to S3
        run: |
          if ls ${{ matrix.folder }}/kics-output/* 1> /dev/null 2>&1; then
            timestamp=$(date +%Y%m%d-%H%M%S)
            for file in ${{ matrix.folder }}/kics-output/*; do
              filename=$(basename "$file")
              echo "Uploading $filename for ${{ matrix.folder }} to S3..."
              aws s3 cp "$file" \
                "s3://$KICS_S3_BUCKET/kics-results/${{ matrix.folder }}/$timestamp/$filename"
            done
          else
            echo "No KICS results found to upload."
          fi
          
       # Enforce security gates before apply
      - name: Fail on critical/high KICS issues
        run: |
           count=$(jq '[.queries[] | select(.severity == "HIGH" or .severity == "CRITICAL")] | length' ${{ matrix.folder }}/kics-output/results.json)
           echo "High/Critical findings: $count"
           if [ "$count" -gt 0 ]; then
             echo "High/Critical KICS findings â€” failing CI"
             exit 1
           fi
           
      - name: Install SonarScanner
        run: |
          wget https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-5.0.1.3006-linux.zip
          unzip sonar-scanner-cli-5.0.1.3006-linux.zip
          mv sonar-scanner-5.0.1.3006-linux sonar-scanner
          echo "$PWD/sonar-scanner/bin" >> $GITHUB_PATH
          
      - name: Run SonarQube Scan
        id: sonar_scan
        continue-on-error: true
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          cd ${{ matrix.folder }}
          sonar-scanner \
            -Dsonar.projectKey=htown-sp-sonarqube-${{ matrix.folder }} \
            -Dsonar.projectName=htown-sp-sonarqube-${{ matrix.folder }} \
            -Dsonar.sources=. \
            -Dsonar.host.url=${{ secrets.SONAR_HOST_URL }} \
            -Dsonar.token=$SONAR_TOKEN \
            -Dsonar.python.version=3 \
            -Dsonar.projectVersion=1.0.${{ github.run_number }} \
            -Dsonar.scm.disabled=false \
            -Dsonar.qualitygate.wait=true \
            -Dsonar.sourceEncoding=UTF-8 \
            | tee ../sonarqube-output.log
            
      - name: Check SonarQube Quality Gate from log
        run: |
          echo "Checking for QUALITY GATE STATUS failure or EXECUTION FAILURE..."
          if grep -q "QUALITY GATE STATUS: FAILED" sonarqube-output.log || grep -q "EXECUTION FAILURE" sonarqube-output.log; then
            echo "SonarQube Quality Gate FAILED"
            exit 1
          else
            echo "SonarQube Quality Gate PASSED"
          fi
          
      - name: Fetch SonarQube Issues
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        run: |
          curl -s -u "${SONAR_TOKEN}:" \
            "${SONAR_HOST_URL}/api/issues/search?componentKeys=$PROJECT_NAME-${{ matrix.folder }}" \
            -o sonarqube-results.json || echo "Failed to fetch SonarQube issues"
            
      - name: Upload SonarQube Issues to S3
        run: |
          if [ -f sonarqube-results.json ]; then
            timestamp=$(date +%Y%m%d-%H%M%S)
            echo "Uploading SonarQube results for ${{ matrix.folder }} to S3..."
            aws s3 cp sonarqube-results.json \
              "s3://$SQ_S3_BUCKET/sonarqube/${{ matrix.folder }}/$timestamp/sonarqube-results.json"
          else
            echo "Issues file not found, skipping upload."
          fi
          
      # Terraform Execution (After Approval)
      - name: Zip Lambda Code and Layer (if applicable)
        run: |
          cd ${{ matrix.folder }}
          if [ -d python ] || [ -d layers ]; then
            mkdir -p build
          fi
          if [ -d lambdas ]; then
            for dir in lambdas/*; do
              [ -d "$dir" ] || continue  # Skip non-directories
              name=$(basename "$dir")
              zip -r "build/${name}.zip" "$dir"
            done
          fi
          if [ -d layers ]; then
            for dir in layers/*; do
              [ -d "$dir" ] || continue
              name=$(basename "$dir")
              zip -r "build/${name}_layer.zip" "$dir"
            done
          fi
          
      - name: Upload Artifacts to S3 and Generate Hashes (if applicable)
        run: |
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out
          echo '{ "lambda_functions": {' > tfvars_out/source_code.tfvars.json
          first=1
          if compgen -G "build/*.zip" > /dev/null; then
            for zip in build/*.zip; do
              name=$(basename "$zip" .zip)
              # Skip layer zip files (ending in _layer.zip)
              if [[ "$name" == *_layer ]]; then
                continue
             fi
             hash=$(openssl dgst -sha256 -binary "$zip" | openssl base64)
             # Upload to S3
             aws s3 cp "$zip" "s3://${{ env.LAMBDA_ARTIFACT_BUCKET }}/lambdas/$zip"
             if [ $first -eq 1 ]; then
               first=0
             else
               echo "," >> tfvars_out/source_code.tfvars.json
             fi
             echo "  \"$name\": { \"source_code_hash\": \"$hash\" }" >> tfvars_out/source_code.tfvars.json
           done
          fi
          echo "} }" >> tfvars_out/source_code.tfvars.json
          
      - name: Upload tfvars_out to Artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out/source_code.tfvars.json
          
      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}
        
      - name: Terraform Validate
        run: terraform validate
        working-directory: ${{ matrix.folder }}
        
      - name: Terraform Plan
        run: terraform plan -var-file="vars/dev.tfvars" -var-file="tfvars_out/source_code.tfvars.json"
        working-directory: ${{ matrix.folder }}
        
  terraform-apply:
    name: Terraform Apply
    needs: [run-ci, detect-folders]
    runs-on: ubuntu-latest
    environment:
      name: dev-apply  #This triggers manual approval
      url: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Download tfvars_out from Artifact
        uses: actions/download-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out
          
      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}
        
      - name: Terraform Apply
        run: terraform apply -auto-approve -var-file="vars/dev.tfvars" -var-file="tfvars_out/source_code.tfvars.json"
        working-directory: ${{ matrix.folder }}
        
      - name: Terraform Apply
        run: terraform apply -auto-approve -var-file="vars/dev.tfvars"
        working-directory: ${{ matrix.folder }}
